# 자료조사 및 개념정리

### 목차

#### [1.머신러닝 전반](##1.머신러닝-전반)

#### [2.NN(Neural Network, 인공 신경망)](##2.neural-network)

#### [3.RNN(Recurrent Neural Network, 순환 신경망)](##rnn(recurrent-neural-network,-순환-신경망))

#### [4.CNN (Convolutional Neural Network, 합성곱 신경망)](##cnn-(convolutional-neural-network,-합성곱-신경망))





## 1.머신러닝 전반

### 1-1. 머신러닝

##### **개요**

- 명시적으로 프로그래밍을 하지 않고도 컴퓨터가 알아서 학습할 수 있는 능력을 갖게 하는 것



**분류**

- 학습방법
  1. 지도 학습(Supervised Learning) : 학습 데이터마다 레이블을 가지고 있다. 각 입력에 대한 출력(정답)의 쌍을 가지고 학습한다.
  2. 비지도 학습(Unsupervised Learning) : 학습 데이터가 레이블을 가지고 있지 않다. 각 요소별로 특징을 찾아내는 방식을 사용한다.
  3. 준지도 학습(Semi Supervised Learning) : 학습 데이터가 약간의 레이블을 가진다.
  4. 강화학습(Reinforcement Learning) : 최종 출력이 바로 주어지지 않고 시간이 지나서 주어지는 경우에 해당한다. ex) 바둑이 승패예측의 경우 finalOutCome이 한번의 action에 의해 나오지 않기 때문에 agent가 state를 보고 여러번의 action을 취해 최대한의 reward를 얻도록 하는 방식을 취한다



### 1-2. 가설함수

##### **개요**

- 머신러닝이랑 가설함수를 반복적으로 수정하여 에러율(cost)의 값을 줄여나가는 과정이다. 따라서 가설함수를 다룬는 방법은 머신러닝의 기초에 근간이 된다.

**공식 소개**

![img](https://lh5.googleusercontent.com/Ci_B2u6ehFF7ewZKBUInQmVZMQEsxLJNpFhzPndIQcoMwdZPFH7p_H_ROPHDSIjH5qC_h7kZv3AogJNGa2oGx_JA_UawWBUS1FDRcrdKLy9fmJF9047l3XrSwjBe_StQ0cLqevRZ)

`이미지 출처:  https://www.edwith.org/boostcourse-dl-tensorflow/lecture/41157/ `

- 가설함수(Hypothesis) : 실제 우리가 만든 모델의 함수를 의미한다.
- Cost : 예측과 실제의 차이를 구하는 함수. 정답과 예측값 차이의 제곱을 전부 더해서 데이터 수로 나는 것이다.
- Gradient Descent(경사 하강) : 경사를 하강시키면서 cost가 최소가 되는 경우를 찾는 알고리즘이다.



**Gradient Descent**

![img](https://lh6.googleusercontent.com/gG6f0Dtr08eJIikOt8lKPbTBF61-HhhgZpW1xOkoqADYRhyuGB_ZT9_xfddjW6L1MSI69Cq2i5frCT323Gt5CoF0MhASwxB9SMj3pgY-mcZUPrJ_N90A2wFfwXx9CgS61I4isvXh)

`이미지 출처:  https://www.edwith.org/boostcourse-dl-tensorflow/lecture/41157/ `

- 위 그래프에서 보이는 것처럼 머신러닝이란 cost가 최저점이 되는 W를 찾는 과정이다.
- 특정 지점에서 W - W*Gradient를 해준값을 다음 지점으로 삼으면 기울기가 원만해지는 방향으로 이동한다. 
- 이동의 특징
  1. 이동할수록 기울기가 점점 완만해진다.
  2. 기울기가 클수록 더 많이 이동한다.



### 1-3. 모델의 종류

##### **모델 소개** 

- Logistic Regression : Classificatoin이라고도 하며, 데이터를 두 분류로 구분하는 선을 만드는 모델이다.
- Linear Regression : 연속적인 데이터의 평균을 구하면서 새로운 데이터의 결과를 맞출 수 있는 범용적인 선을 만드는 모델이다.
- Softmax Regression : 2개 중 하나를 선택하는 것이 아닌 다중분류를 위한 학습모델이다. 주로 행렬을 사용하여 여러개의 가중치를 가지고 각각의 결과를 출력하는 함수를 만들어 모델을 학습시킨다.
- Support Vector machines : Classification처럼 데이터를 분류하는 모델이지만 기존과 차이점을 가진다. 기존의 ERM(경험적 위험 최소화)를 사용했으나 svm은 SRM(구조적 위험 최소화)를 통해 보이지 않는 데이터의 위험을 최소하하여 일반화된 오류를 최소화한다. 



**Logistic vs Linear** 

![img](https://lh3.googleusercontent.com/Lh6z3jzTaSNHAhE700tC7WywMNhqL9R8uAAF7RJoUu7SNq2CLN5FcWH5bzxWIWsk4IsmrXqGuawowf6cXRU74cI2tqvsHuTh8maWvNTY-8BidExxpdOsqWKih4bE_ZQ3cd-WL95T)

`이미지 출처:  https://www.edwith.org/boostcourse-dl-tensorflow/lecture/41157/ `

- Logistic Regression은 입력값을 두 분류로 구분하는 것이고 Linear Regression은 연속적인 데이터를 위한 모델이다. Digit와 Analogue의 차이와 비슷하다.





## 2.Neural Network

### 2-1. 기존과의 차이점

**기존의 판별함수**

![img](https://lh6.googleusercontent.com/uk9rYVmANii1HWIeV2gbA3h_VYknpWsz9lPIIzjjP3A_CN7lSe5wJAazMFZt4cJ8Fg1zb89jGiafS3SLZC68k8_FCy-R3elypp6mFe62oPHCrEaO5fLhhwkPqMF4B8UXFjuXYgKA)

![img](https://lh6.googleusercontent.com/mHMril_ne7xVF7_H1RYDCmE5vhWHYpPiiZgcYeMEzeyJE9UTKQZU7ElScMIe4xT-3e1A4DhK4-NhBzYFXm_9VJyKEzVRMPHHGxmcfgHSyvAIBCw9Hri7p9JKGwaXJANr4rXwm7EP)

* 기존의 선형판별함수드의 x들에 의해 가중치w가 입력되고 가중치를 함계하여 함수를 실행시키는 방법에서 오직 Single Layer Perceptron만 사용하였다.
* 따라서 선형으로 분리 가능한 문제만 풀 수 있다는 단점이 있었다.



**NN의 차별성**

![img](https://lh3.googleusercontent.com/_O-9QRU_7y9D8_Gq1sW8G4LCmrl9NZ3fdQxRxk0x9CY2SAPi3v_mrmDXvBEnWNnnwGKMWkuLPiYfHqyE-NSXc6KdkwUPtlC4atNGFs71OVGjdRotAZp730hU97UwlsigHPUrSe-L)

* 사진과 같이 NN은 다중레이어를 가지기 때문에 XOR을 비롯한 비선형 문제들을 풀 수 있다.



### 2-2. NN의 특징

**대표적 특징**

* 뉴런으로 된 네트워크이며 이론적으로 비선형 문제를 포함한 모든 문제를 해결할 수 있다.

* 세 개 이상의 레이어로 구성되어 있고 input - hidden - output레이어로 구성되있다.

* 해석하기가 어렵고 파라미터 세팅이 어렵다.

  

##### **Hidden 레이어와 노드의 역할**

- Hidden Layer : input space를 몇 개의 sub space로 나누어준다.
- Hidden Node : 각 sub-space의 decision rule이며 Hidden Layer가 다중이면 Hidden Node도 다중이 된다.
- Hidden Layer가 너무 많은 경우 sub space로 나뉘어 overfitting된다.



**NN 디자인**

- 충분한 수의 트레이닝 데이터가 필요하다.
- 복잡한 네트워크가 강력한 모델을 만들고 성능을 높인다.
- Layer, Hidden Node, Activation function을 고려해야 한다.



**NN의 최적화**

- NN은 가중치를 업데이트할 훈련 알고리즘이 없고 네트워크를 최적화하기가 힘들다.
- Backpropagation algorithm
  - NN에 사용되는 가중치 최적화 알고리즘이다.
  - 파라미터가 점점 업데이트를 할 때 발생하는 에러를 줄이기 위해 가중치를 업데이트한다.
  - 공식은 다음과 같다.
  - ![img](https://lh4.googleusercontent.com/8oJs6XeLne4LCjCfUCxRCmm_upxXY1x1RnmXrvEERxsN6Ttbs0z1NWIisEoxE9wdM9m_WPC47awTaFeW6crbq2JS0UOGSiM0rO3Adq4tkQHbytzdiqNBzXRrn3-_QyX8j6SqR1sy)
  - feedforward : 패턴을 입력장치에 넣어 네트워크 신호를 통해 출력유닛을 얻는다.
  - 학습방법 : Supervised Learning을 통해 입력후 계산된 출력과 원하는 출력간의 거리가 줄어들도록 Network parameters를 수정한다. 
  - 결론 : backpropagation을 이용해 output에서 hidden, hidden에서 input을고 가는(잘 보면 역순으로 진행된다.) 방향으로 가중치를 설정한다(error term을 미리 정의). 이를 통해 Multi Layer perceptron이 가능해진다.



**NN에서의 Gradient Descent**

- local optima(최적값)으로 인해 overfitting되기가 쉽다.

- local optima를 피하기 위해서는

  1. 가중치 W의 시작점을 랜덤하게 초기화한다.

  2. 아래 그림에서 J(w)와 J(w)의 경사가 세타 이하일 때를 보고 validation error를 학습시켜 learning curves를 만드는 stopping criterion(정지 기준)을 정한다.

     ![img](https://lh3.googleusercontent.com/ZDmKfKk4U9PcPZzq649urBfksIf9GQdEeq_-l1S_qzniI8aHdsKQu2UVTmtpMZoZB7_v8Xv7wDpye-svLo6csj00QdElxd5S6GlqmZ-RVNsxmRj5miTN7xfG_yfiprKczdMsf9uP)



## 3. RNN(Recurrent Neural Network, 순환 신경망)

### 3-1. 개요

- 기존의 신경망은 Feed Forward NN으로써 은닉층의 함수들이 오직 출력층 방향으로만 향했으나, RNN은 은닉층 노드의 결과가 다음 Hidden Node로 가면서 동시에 출력층 방향으로 보낸다.



### 3-2. 구조

![img](https://lh5.googleusercontent.com/MJXzuyGBHIb6fk9RWBrUM1qHetJajkIoxsQGrKLjZO0RW-pWlcEgNPDXN7uVYgox-4K7EyZ6yl11uwoAB1AmeXOQvyeHCQ2h2T5MYv-EGkkYYfjjY6IOANUDpcJn7CHC7mqASRfN)

`이미지 출처:  https://wikidocs.net/22886 `

- 사진에 보이는 것처럼 RNN은 출력층과 입력층이 하나뿐인 기존의 방식과 다르게 Hidden Layer의 각 노드가 입력층 및 출력층과 연결되어 있다.
- 이러한 특성으로 인해 은닉층의 각 노드들은 이전 셀 값을 기억한다는 의미에서 '메모리 셀', 'RNN 셀'로 불린다.

![img](https://lh3.googleusercontent.com/X26Z0Jz_IPLV3-qDoDzEFPuftDh0bMZYS-TZD8hMsQlopnP1LW0uk4RYADKbAZ94v0EqOMp__GVpriiyAwYZPRfhYjqopVtGcDX1UjtDWnTlGvre_aHhdTdjpsaAxv8-TKuefNxt)

`이미지 출처:  https://wikidocs.net/22886 `

- 또한, 출력벡터와 입력벡터의 개수를 조절하는 것으로 다양한 분야에 사용할 수 있다.
- 모델별 예시는 다음과 같다.
  1. one to many : 하나의 이미지 입력에 대해 사진의 제목을 출력하는 '이미지 캡셔닝'.
  2. many to one : 입력 문서의 긍정,부정 여부를 판별하는 '감성분류' 혹은 각 단어들을 조합하여 스팸메일 여부를 판별하는 등 classification에 사용된다.
  3. many to many : 여러 단어로 된 문장을 여러 단어로 된 번역본으로 바꾸는 '번역기' 혹은 마찬가지로 문장에 문장으로 대답하는 '챗봇'등이 있다.



### 3-3. 수식정의

![img](https://lh3.googleusercontent.com/sB5yef17-_jkqlP4jpj9tUKX4RAk0L3MvZLLcgdKud2IoerByG9ZrQFkykRcT_o82wyfqBK7N9OZGjmcBzuXPXNNdQQtMUj3sa0AautFAG1Z4lDPz7T4LsJv7VY1YE0mic8bS-RQ)

`이미지 출처:  https://wikidocs.net/22886 `

- RNN의 각 메모리 셀은 2개의 가중치를 가진다. 
  1. 입력벡터
  2. 이전 메모리셀의 결과
- 따라서, 메모리셀의 상태값과 출력층의 결과값의 수식은 다음과 같다.
  1. 메모리 셀 상태값 : ht=tanh(Wxxt+Whht−1+b)
  2. 출력층 결과값 : yt=f(Wyht+b)
- 이러한 다단계 계산으로 인해 Softmax regression처럼 RNN은 여러개의 가중치를 표현하고 계산하기 위해 행렬을 사용한다.





##  4. CNN (Convolutional Neural Network, 합성곱 신경망)

###  4-1. 개요

- 이미지 분류에서 가장 널리 사용되는 신경망

- 세 종류의 layer로 구성

  - Convolution Layer, Pooling Layer 
    - 주로 앞쪽에 배치
    - feature extraction - 특성 추출 담당

  - Fully-connected Layer
    - 실질적 분류 판단 담당

- ##### 기원

  시각 피질 안의 많은 뉴런이 작은 **local receptive field**(국부 수용영역)들이 시야의 일부 범위 안에 있는 시각 자극에만 반응,  서로 겹칠수 있으며, 이렇게 겹쳐진 **수용영역**들이 전체 시야를 이루게 된다.

  =>  고수준의 뉴런이 이웃한 저수준의 뉴런의 출력에 기반

- ##### 필터 (Filter)

  - 위의 설명된 **수용영역** , 합성곱 층에서 입력데이터에 필터를 적용하여 필터와 유사한 이미지의 영역을 강조하는 **특성맵(feature map)**을 출력하여 다음 층(layer)으로 전달

  

- ##### 패딩 (Padding)

  -  합성곱 연산을 수행하기 전, 입력데이터 주변을 특정값으로 채워 늘리는 것을 말한다.
     -  주로 합성곱 계층의 출력이 입력 데이터의 공간적 크기와 동일하게 맞춰주기 위해 사용한다.

  ![img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile22.uf.tistory.com%2Fimage%2F9916C23F5BC97EEE31EF65)

  `출처: https://excelsior-cjh.tistory.com/180 [EXCELSIOR] `

- ##### 스트라이드 (Stride)

  - 필터가 이동할 간격 
  - 출력 데이터의 크기를 조절하기 위해 사용 





### 4-2. 2차원 합성곱 연산

![img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99A440405BC97EDC20626B)

`이미지 출처:  https://excelsior-cjh.tistory.com/180 `



입력 데이터: (4, 4) 

필터: (3, 3) 

- 합성곱 연산은 필터의 윈도우를 일정한 간격(stride)으로 이동해가며 계산한다.

- 왼쪽 위부터 필터 와 입력 데이터의 값들을 겹쳐서 곱한값들을 전부 더한다. 

  `예시: 1*2+2*0+3*1+0*0+1*1+2*2+3*1+0*0+1*2` = 15 

  이동하며 위를 반복 

  반복 횟수 

  == 출력 데이터

####  출력 데이터의 크기를 구하는 식

- (H,W) : 입력 크기 (input size)

- (FH, FW) : 필터 크기 (filter/kernel size)

- S : 스트라이드 (stride)

- P : 패딩 (padding)

- (OH, OW) : 출력 크기 (output size)

$$
(OH, OW) = (\frac{H+2P-FH}{S}+1, \frac{W+2P-FW}{S}+1)
$$

`**출력크기가 정수가 아닌 경우는 반올림 `





### 4-3. 3차원 합성곱 연산

- 이미지 데이터의 경우 3차원(세로, 가로, 채널)의 형상을 가짐

- 합성곱 연산을 수행할 때, **입력 데이터의 채널 수와 필터의 채널수가 같아야 한다**.



![img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99C185405BC97F4D1EE8D1)

- 하나의 필터를 이용해 합성곱 연산을 하게 되면 하나의 채널을 가진다.





#### 수식 표현

$$
Z_i,j,k = b_k+\sum_{u=0}^{f_h-1}\sum_{v=0}^{f_w-1}\sum_{k'=0}^{f_{n'}-1}x_{i',j',k'}*w_{u,v,k',k}
 \ \ \  with \begin{cases}
 i'=i*s_h+u\\[2ex]
 j'=j*s_w+v
 \end{cases}
$$

- z_i,j,k: 합성곱층(층)의 -특성 맵에서 행, 열에 위치한 뉴런의 출력

- s_h, s_w: 높이(), 너비() 의 스트라이드

- f_h, f_w : 필터(receptive field)의 높이()와 너비()

- f_n': 이전 층 ()에 있는 특성맵의 수

- x_i',j',k': 층의 행, 열,  특성맵에 있는 뉴런의 출력

- b_k: -특성맵의 편향(bias) → 이 편향을 -특성맵의 전체 밝기를 조절하는 것으로 볼 수 있음

- w_u,v,k',k: 층의 -특성맵에 있는 필터와 연관된 행, 열, 특성맵의 뉴런의 가중치 



`이미지 및 개념 출처: https://excelsior-cjh.tistory.com/180`